"""
å¼•ç”¨æ•°å¢—åŠ æ•°ã®åˆ†å¸ƒèª¿æŸ»ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
config.pyã®è¨­å®šã«åŸºã¥ã„ã¦ã€ç›´è¿‘ã®å¼•ç”¨å¢—åŠ æ•°ã®åˆ†å¸ƒã‚’é›†è¨ˆã™ã‚‹ã€‚
"""
import logging
import argparse
from collections import Counter
import signal
import sys
import time

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import config
from pubmed_fetcher import search_pmids, fetch_article_details
from opencitations import get_citation_increase
from dictionary import get_mesh_query

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)

# ä¸­æ–­ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
interrupted = False

def signal_handler(sig, frame):
    global interrupted
    logger.info("\nä¸­æ–­ã‚·ã‚°ãƒŠãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚é›†è¨ˆã‚’å®Œäº†ã—ã¦çµæœã‚’è¡¨ç¤ºã—ã¾ã™...")
    interrupted = True

signal.signal(signal.SIGINT, signal_handler)

def analyze_distribution(limit: int = 200):
    """
    æŒ‡å®šä»¶æ•°ã®è«–æ–‡ã«ã¤ã„ã¦å¼•ç”¨å¢—åŠ æ•°ã‚’èª¿æŸ»ã—ã€åˆ†å¸ƒã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    """
    # 1. PubMed æ¤œç´¢
    fields = config.DEFAULT_FIELDS
    logger.info(f"èª¿æŸ»å¯¾è±¡åˆ†é‡: {fields}")
    logger.info(f"æœ€å¤§èª¿æŸ»ä»¶æ•°: {limit}")

    mesh_queries = []
    for field in fields:
        q = get_mesh_query(field)
        if q:
            mesh_queries.append(q)

    if not mesh_queries:
        logger.error("æœ‰åŠ¹ãªã‚¯ã‚¨ãƒªãŒã‚ã‚Šã¾ã›ã‚“")
        return

    all_pmids = []
    for query in mesh_queries:
        # æ¤œç´¢ä»¶æ•°ã‚’åˆ¶é™ã—ã¦å–å¾—
        pmids = search_pmids(query, retmax=limit*2) # é‡è¤‡é™¤å»ç­‰ã‚’è€ƒæ…®ã—ã¦å¤šã‚ã«
        all_pmids.extend(pmids)
    
    # é‡è¤‡é™¤å» & ä»¶æ•°åˆ¶é™
    target_pmids = list(dict.fromkeys(all_pmids))[:limit]
    logger.info(f"æ¤œç´¢ãƒ’ãƒƒãƒˆä»¶æ•°: {len(all_pmids)} -> èª¿æŸ»å¯¾è±¡: {len(target_pmids)}ä»¶")

    if not target_pmids:
        logger.info("å¯¾è±¡è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
    logger.info("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
    articles = fetch_article_details(target_pmids)
    logger.info(f"{len(articles)}ä»¶ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")

    # 3. å¼•ç”¨å¢—åŠ æ•°é›†è¨ˆ
    increase_counts = []
    stats = {
        "total": len(articles),
        "no_doi": 0,
        "api_error": 0,
        "zero_increase": 0,
        "positive_increase": 0,
        "increases": []
    }

    logger.info("OpenCitations APIã§å¼•ç”¨æ•°èª¿æŸ»ä¸­... (Ctrl+Cã§ä¸­æ–­ã—ã¦çµæœè¡¨ç¤º)")
    
    try:
        for i, article in enumerate(articles, 1):
            if interrupted:
                break
                
            doi = article.get("doi")
            if not doi:
                stats["no_doi"] += 1
                logger.debug(f"[{i}/{len(articles)}] DOIãªã—: {article['pmid']}")
                continue

            increase = get_citation_increase(doi)
            
            if increase is None:
                stats["api_error"] += 1
                logger.debug(f"[{i}/{len(articles)}] APIã‚¨ãƒ©ãƒ¼: {doi}")
                continue

            stats["increases"].append(increase)
            
            if increase == 0:
                stats["zero_increase"] += 1
                # 0ä»¶ã®å ´åˆã¯ãƒ­ã‚°ã‚’çœç•¥ï¼ˆé‡ãŒå¤šã„ã®ã§ï¼‰
                if i % 10 == 0:
                     logger.info(f"[{i}/{len(articles)}] PMID:{article['pmid']} Inc:0 (é€²æ—ç¢ºèªç”¨)")
            else:
                stats["positive_increase"] += 1
                logger.info(f"[{i}/{len(articles)}] ğŸ“ˆ å¢—åŠ ã‚ã‚Š! PMID:{article['pmid']} Inc:+{increase} Title:{article['title'][:30]}...")

            # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™éµå®ˆ (opencitations.py å†…ã§ã‚‚waitã—ã¦ã‚‹ãŒå¿µã®ãŸã‚)
            # time.sleep(0.5) 
            
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
    
    # 4. é›†è¨ˆçµæœè¡¨ç¤º
    display_results(stats)

def display_results(stats):
    total_processed = len(stats["increases"]) + stats["no_doi"] + stats["api_error"]
    increases = stats["increases"]
    
    print("\n" + "="*60)
    print("å¼•ç”¨å¢—åŠ æ•° åˆ†å¸ƒèª¿æŸ»çµæœ")
    print("="*60)
    print(f"èª¿æŸ»ç·æ•°: {total_processed} ä»¶")
    print(f"  - DOIãªã—: {stats['no_doi']} ä»¶")
    print(f"  - APIã‚¨ãƒ©ãƒ¼: {stats['api_error']} ä»¶")
    print(f"  - æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿æ•°: {len(increases)} ä»¶")
    print("-" * 60)
    
    if not increases:
        print("æœ‰åŠ¹ãªå¼•ç”¨ãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # åˆ†å¸ƒé›†è¨ˆ
    counter = Counter(increases)
    sorted_keys = sorted(counter.keys())
    
    print("ã€å¼•ç”¨å¢—åŠ æ•°ã®åˆ†å¸ƒã€‘")
    for k in sorted_keys:
        count = counter[k]
        bar = "â–ˆ" * (count * 50 // len(increases)) if len(increases) > 0 else ""
        if count * 50 // len(increases) == 0 and count > 0:
            bar = "â–"
        print(f"  å¢—åŠ æ•° {k:2d}: {count:3d} ä»¶ ({count/len(increases)*100:5.1f}%) {bar}")

    print("-" * 60)
    print("ã€é–¾å€¤åˆ¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€‘")
    cumulative = 0
    # å¤§ãã„æ–¹ã‹ã‚‰ç´¯ç©
    for k in sorted(sorted_keys, reverse=True):
        if k <= 0: break
        cumulative += counter[k]
        print(f"  é–¾å€¤ {k} ä»¥ä¸Š: {cumulative:3d} ä»¶ é€šçŸ¥å¯¾è±¡")

    print("="*60)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¼•ç”¨å¢—åŠ æ•°åˆ†å¸ƒèª¿æŸ»")
    parser.add_argument("--limit", type=int, default=100, help="èª¿æŸ»ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°")
    args = parser.parse_args()
    
    analyze_distribution(limit=args.limit)
